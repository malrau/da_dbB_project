{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b658c051",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h2>Online learning platform database - Cassandra</h2>\n",
    "    <h3>Methodologies applied for loading the data and performing the queries</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544d0b6",
   "metadata": {},
   "source": [
    "<h3>Preliminary operations: import csv files into Cassandra (<code>COPY FROM</code>)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf09d3",
   "metadata": {},
   "source": [
    "A csv file can be imported into a Cassandra table via the <code>COPY FROM</code> CQL <a href = 'https://cassandra.apache.org/doc/4.1/cassandra/tools/cqlsh.html#copy-from'>command</a>, although for large files it is suggested, to rely on <a href = 'https://cassandra.apache.org/doc/latest/cassandra/operating/bulk_loading.html'><i>bulk</i> loading</a>, an operation that can be performed either with the <code>sstableloader</code> command or with the <code>nodetool import</code> command. Neither of the two is a DML command and they need being executed from the Cassandra shell.<br>A major limitation of these tools resides in the fact that they can import external files in the form of <i>sstables</i>, not directly csv files. Hence, <i>bulk-loading</i> external data that is not in sstable form, should first be stored into sstables, a process that requires using the <code>CQLSSTableWriter</code> Java class. After experimenting with a file with a couple of thousands of rows, I have resolved to rely on the <code>COPY FROM</code> CQL command.\n",
    "<br>\n",
    "<h4>\n",
    "Syntax\n",
    "</h4><br>\n",
    "To use the <code>COPY FROM</code> command we must first select the keyspace (<code>USE <i>mykeyspace</i></code>) of the table that we want the csv data to be imported into. The table must exist within the selected keyspace and its schema must replicate the header of the csv file and related data types.\n",
    "<br>\n",
    "<code>\n",
    "    COPY <i>mytable</i> FROM '<i>mycsvfile.csv</i>';\n",
    "</code>\n",
    "<br>\n",
    "Equivalently, without the need to first select the keyspace, we could specify the table as a keyspace attribute: <br>\n",
    "<code>\n",
    "    COPY <i>mykeyspace.mytable</i> FROM '<i>mycsvfile.csv</i>';\n",
    "</code>\n",
    "<br>\n",
    "It is to be considered that when we perform a <code>CREATE TABLE</code> CQL command there is no guarantee that the output table will keep the inputted column order. So, although in table creation we might have specified the same column order of the first row of the csv (header), when importing it is advisable to specify the column list (within round brackets after the table name) in the same order as it appears in the csv header.\n",
    "<br>\n",
    "<code>\n",
    "    COPY <i>tablename(column1, column2, ...)</i> FROM '<i>mycsvfile.csv</i>';\n",
    "</code>\n",
    "<h4>\n",
    "Options\n",
    "</h4><br>\n",
    "Options are specified after the keyword <code>WITH</code> and separated by the keyword <code>AND</code>. Options are useful to change the CSV format. In fact, by default, Cassandra expects the CSV data to consist of fields separated by commas (,), records separated by line separators (a newline, \\r\\n), field values enclosed in double-quotation marks (\"\") and a header row not to be present.\n",
    "<br>\n",
    "<br>\n",
    "- <code><b>HEADER</b></code><br>\n",
    "If the first row of the csv file contains the column names, we must specify <code>HEADER = true</code> after the filename, so that it is ignored when importing.\n",
    "<br>\n",
    "<code>\n",
    "    COPY <i>tablename(column1, column2, ...)</i> FROM '<i>mycsvfile.csv</i>' \n",
    "        WITH HEADER = true;\n",
    "</code>\n",
    "<br>\n",
    "- <code><b>DELIMITER</b></code><br>\n",
    "To avoid problems in column identification, it is also advisable to specify the character used within the csv file to separate columns.\n",
    "<br>\n",
    "<code>\n",
    "    COPY <i>tablename(column1, column2, ...)</i> FROM '<i>mycsvfile.csv</i>' \n",
    "        WITH HEADER = true AND DELIMITER = ',';\n",
    "</code>\n",
    "<br>\n",
    "- <code><b>QUOTE</b></code><br>\n",
    "If there are commas within column values, column values usually come written within double quotes. If a different enclosing characyer is used, it must be specified with the <code>QUOTE</code> option.\n",
    "<br>\n",
    "<code>\n",
    "    COPY <i>tablename(column1, column2, ...)</i> FROM '<i>mycsvfile.csv</i>' \n",
    "        WITH HEADER = true AND DELIMITER = ',' AND QUOTE = '\"';\n",
    "</code>\n",
    "<br>\n",
    "However, in my datasets, usually I don't use double quotation to enclose field values, so to preserve data types. Hence, only strings that contain commas are enclosed in double quotes. This may cause <code>COPY FROM</code> to experience troubles because, if <code>QUOTE</code> <b>is used</b>, not every field value comes with this kind of enclosure, while, if <code>QUOTE</code> <b>is not used</b>,<code>COPY FROM</code> will likely fail in identifying the correct number of field when reaching a row where some values are double quoted. This is likely to happen for the <i>courseName</i> field: course names may include commas. To increase compatibility, it is then advisable to double quote dictionary values when creating the data with Python and leave to the DBMSs the task of identifying the correct data type.\n",
    "<br>\n",
    "<h4>\n",
    "Notes when running Cassandra from a Docker container\n",
    "</h4><br>\n",
    "In loading the csv file from the local machine, we must take into account the usual feature of Docker virtual environments. They come with a file system of their own, so a DBMS run from within a container has access to this file system, not to the local machine file system. Hence, the csv file must be imported into the container via the usual <code>docker cp</code> command.\n",
    "<br>\n",
    "<code>\n",
    "    docker cp path/mycsvfile.csv container:/path\n",
    "</code>\n",
    "\n",
    "Contrary to what has been specified in the MySQL and MongoDB cases, I run the <code>COPY FROM</code> command from <i>cqlsh</i>, not from a system folder or from a container shell. Also, the file has been copied in the root folder of the container file system, hence I don't need to include a path for the csv file, I just specify its name within single quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65acd0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ebe4135",
   "metadata": {},
   "source": [
    "<h3>Python - Cassandra interaction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad76a4",
   "metadata": {},
   "source": [
    "Interaction between a Python API and a Cassandra DBMS requires the installation of a specific driver. The usual list of drivers for various programming languages is provided in the <a href = 'https://cassandra.apache.org/doc/latest/cassandra/getting-started/drivers.html'>Client drivers</a> web page of the Cassandra website: <a href = 'https://docs.datastax.com/en/developer/python-driver/3.28/'>Datastax Python driver</a> is the one suggested for a Python programming environment.<br>After having installed the driver, it can be imported into a Python environment the usual way. In particular, we need to setup an instance of Cluster for the Cassandra cluster we want to interact with, so we first import the <code>Cluster</code> class from the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5469de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddef33",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Establishing a connection to a Cassandra cluster\n",
    "</h4><br>\n",
    "When instantiating a cluster, the default behaviour of the driver is to try to connect to a Cassandra instance on the local machine. The optional local machine's name (<i>localhost</i>) or ip address (<i>127.0.0.1</i>) can be specified as an argument in the form of a string element of a Python list. Alternatively, the ip addresses of the nodes in the cluster can be provided as string elements of a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294c998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent syntaxes\n",
    "#cluster = Cluster()\n",
    "#cluster = Cluster(['localhost'])\n",
    "cluster = Cluster(['127.0.0.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804b081",
   "metadata": {},
   "source": [
    "Cluster instantiation is not enough to establish a connection to the nodes: establishing a connection requires the creation of a <code>session</code> class via the <code>connect()</code> method of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b869c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e877c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Choosing a database (keyspace)\n",
    "</h4><br>\n",
    "A list of available databases (Cassandra keyspaces) is stored as a dictionary within the <code>cluster.metadata.keyspaces</code> object. The keyspaces' names are the dictionary keys, while the values represent keyspaces' memory locations. Iterating over the dictionary keys lets us easily print out the desired information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae0a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of keyspaces in the Cassandra instance:\n",
      "\n",
      "system_auth\n",
      "system_schema\n",
      "dbb_cassandra_test\n",
      "system_distributed\n",
      "system\n",
      "system_traces\n",
      "system_views\n",
      "system_virtual_schema\n"
     ]
    }
   ],
   "source": [
    "print('List of keyspaces in the Cassandra instance:\\n')\n",
    "for keyspace in cluster.metadata.keyspaces.keys():\n",
    "    print(keyspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed13468",
   "metadata": {},
   "source": [
    "A session can take a keyspace name as argument, in that case that is the default keyspace for queries executed through that session. A keyspace can also be assigned to the session via the <code>set_keyspace()</code> method or passing the CQL <code>USE <i>mykeyspace</i></code> command to the <code>execute()</code> method of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9802efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.execute('USE dbb_cassandra_test')\n",
    "session.set_keyspace('dbb_cassandra_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f459600",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Executing a query\n",
    "</h4><br>\n",
    "Queries can be performed by passing the query string to the <code>execute()</code> method of the session (we must have previously chosen the keyspace). Since Cassandra is focused on performance, it imposes some limitations on the query mechanisms. A first limitation regards filtering. To avoid scanning a potentially large dataset thus negatively impacting  on performance, the <code>WHERE</code> clause is not executed and a message is returned:<br>\n",
    "\n",
    "    Error from server: code=2200 [Invalid query] message=\"Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING\"\n",
    "\n",
    "If we want to allow filtering despite its potential impact on performance, either we append the <code>ALLOW FILTERING</code> keywords to the query string, or we create an index on the column we want to use for filtering prior to performing the desired query:\n",
    "<code>\n",
    "    CREATE INDEX '<i>indexName</i>' ON '<i>tableName</i>'('<i>columnName</i>');\n",
    "</code>\n",
    "After having created the index the query can be performed by applying the <code>WHERE</code> clause to the column on which the index has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bec69f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x108e05480>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute('CREATE INDEX query1Index ON smalldb(courseid);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0693cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = session.execute('SELECT studentID, firstName, lastName FROM smalldb WHERE courseid = \\'192\\';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7c0dd",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Displaying a query result\n",
    "</h4><br>\n",
    "The query result is a set of rows in the form of named tuples. Each element has a name corresponding to its projected column name and a value. We can access the values by position, so we can display them by attaching an index to the elements (rows) of the result set we are looping over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5eeb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653 Émile Nicolas\n",
      "1237 Ana Narušis\n",
      "415 Vigilija Gaižauskas\n",
      "177 Narciso Ferrán\n",
      "1948 Ingeborg Amundsen\n",
      "1208 Arthur Laroche\n",
      "1650 Nath Nicolas\n",
      "447 Casandra Arenas\n",
      "38 Sarah Lara\n",
      "1825 Cathrine Lie\n",
      "664 Ledün Soylu\n",
      "2 Custodia Hidalgo\n",
      "320 Patrícia Leite\n"
     ]
    }
   ],
   "source": [
    "resSet = set()\n",
    "for row in res:\n",
    "    resSet.add(row)\n",
    "for rowrow in resSet:\n",
    "    print(rowrow[0], rowrow[1], rowrow[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef04f2",
   "metadata": {},
   "source": [
    "After having accessed the query result, the memory used by the object result is freed, and the query result cannot be accessed any longer. The query must be performed again to access the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e7d2c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x104293e50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3800e6",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Measuring and displaying the query execution time\n",
    "</h4>\n",
    "<h4>\n",
    "- method 1: <code>time()</code>\n",
    "</h4><br>\n",
    "To display the query execution time we can use the Python <a href = 'https://docs.python.org/3/library/time.html'><i>time</i></a> module and its <code>time</code> function. The function returns the system time at a floating point precision, so the query execution time can be measured as a large number of fractions of a second. It is sufficient to assign the time before the query execution to a variable and the time after the query execution to another variable. The difference between the two variables will measure the query execution. Obviously, the time for the Python API to connect to the Cassandra cluster and the time to return to the Python API after the query execution will be summed up to the query execution time at the DBMS level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81b36cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484.691858291626\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "before = time.time()\n",
    "res1 = session.execute('SELECT studentID, firstName, lastName FROM smalldb WHERE courseid = \\'192\\';')\n",
    "after = time.time()\n",
    "print((after - before) * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc8b99",
   "metadata": {},
   "source": [
    "<h4>\n",
    "- method 2: <code>duration</code>\n",
    "</h4><br>\n",
    "If the <code>trace</code> option is set to true within the <code>session.execute()</code> method, information on the executed query is registered and available after query completion. Among these information, the total query duration can be reported in seconds by using the <code>total_seconds()</code> method. By multiplying the result by 1000 we get the query execution time in milliseconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23675f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547.749\n"
     ]
    }
   ],
   "source": [
    "res2 = session.execute('SELECT studentID, firstName, lastName FROM smalldb WHERE courseid = \\'192\\';', trace = True)\n",
    "print(res2.get_query_trace().duration.total_seconds() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e3e0a",
   "metadata": {},
   "source": [
    "<h4>\n",
    "- chosen method: <code>time()</code>\n",
    "</h4><br>\n",
    "In order to use a uniform method to get query execution times across all the DBMSs used in the project, I resolve to use the <code>time()</code> function of the Python <i>time</i> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c595d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
